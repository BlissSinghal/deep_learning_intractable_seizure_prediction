{"cells":[{"cell_type":"markdown","source":["#Setting Up"],"metadata":{"id":"oV41w-3bk-SL"}},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24360,"status":"ok","timestamp":1686170768345,"user":{"displayName":"Bliss Singhal","userId":"00844180374647156561"},"user_tz":420},"id":"Vb-rTt6hlm8F","outputId":"14d2704d-332e-4d18-f8a7-c0af774fa716"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1686170768345,"user":{"displayName":"Bliss Singhal","userId":"00844180374647156561"},"user_tz":420},"id":"-bWUhzzvlyLu","outputId":"9ebfe31d-70de-4de8-bd8c-ebafebc711ea"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive\n"]}],"source":["%cd drive"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1686170768346,"user":{"displayName":"Bliss Singhal","userId":"00844180374647156561"},"user_tz":420},"id":"2ctkR6V8mTod","outputId":"0561e149-fb5b-4450-ef3f-6cecfcd08567"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/Shareddrives/blisssinghal-dataset\n"]}],"source":["%cd Shareddrives/blisssinghal-dataset/"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9003,"status":"ok","timestamp":1686170777344,"user":{"displayName":"Bliss Singhal","userId":"00844180374647156561"},"user_tz":420},"id":"J2rRkVb2meQG","outputId":"882a1730-b396-4501-b9ab-32346f4038df"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting mne\n","  Downloading mne-1.4.2-py3-none-any.whl (7.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m53.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.10/dist-packages (from mne) (1.22.4)\n","Requirement already satisfied: scipy>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from mne) (1.10.1)\n","Requirement already satisfied: matplotlib>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from mne) (3.7.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from mne) (4.65.0)\n","Requirement already satisfied: pooch>=1.5 in /usr/local/lib/python3.10/dist-packages (from mne) (1.6.0)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from mne) (4.4.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from mne) (23.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from mne) (3.1.2)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.4.0->mne) (1.0.7)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.4.0->mne) (0.11.0)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.4.0->mne) (4.39.3)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.4.0->mne) (1.4.4)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.4.0->mne) (8.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.4.0->mne) (3.0.9)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.4.0->mne) (2.8.2)\n","Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.5->mne) (1.4.4)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.5->mne) (2.27.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->mne) (2.1.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.4.0->mne) (1.16.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.4)\n","Installing collected packages: mne\n","Successfully installed mne-1.4.2\n"]}],"source":["%pip install mne"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"ut_9eCEUlZkt","executionInfo":{"status":"ok","timestamp":1686170785231,"user_tz":420,"elapsed":7891,"user":{"displayName":"Bliss Singhal","userId":"00844180374647156561"}}},"outputs":[],"source":["import glob\n","import numpy as np\n","import pandas as pd\n","import mne\n","import os\n","import matplotlib.pyplot as plt\n","import sklearn \n","import keras\n","import pickle\n","import pickletools\n","import sklearn.neighbors\n","import sklearn.metrics\n","from sklearn.preprocessing import MinMaxScaler"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"8VOiUn883E1N","executionInfo":{"status":"ok","timestamp":1686170785232,"user_tz":420,"elapsed":15,"user":{"displayName":"Bliss Singhal","userId":"00844180374647156561"}}},"outputs":[],"source":["from keras.layers import (LSTM, Dense, RNN, Flatten, Dropout, SimpleRNN, Conv1D)\n","from keras.callbacks import ReduceLROnPlateau\n","from keras import regularizers\n"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"qrOtpRDjrSY2","executionInfo":{"status":"ok","timestamp":1686170792643,"user_tz":420,"elapsed":7425,"user":{"displayName":"Bliss Singhal","userId":"00844180374647156561"}}},"outputs":[],"source":["import torch\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","import time\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C7-ULhqewgMi"},"outputs":[],"source":["# load and preprocess the energy dataset: \n","def convert_col_into_float(df, list_cols):\n","    for col in list_cols:\n","        df[col] = df[col].astype(str)\n","        df[col] = df[col].str.replace(',', '.')\n","        df[col] = df[col].astype(np.float32)\n","    return df\n","df = pd.read_csv(\"energydata_complete.csv\", index_col='date', parse_dates=['date'])\n","print(df.head())\n","list_cols = list(df.columns)\n","# gathers 10-min measurements of household appliances energy consumption (20 first features), coupled with local meteorological data. (8 last features)\n","print(\"dataset variables\", list_cols)\n","df = convert_col_into_float(df, list_cols)\n","data = df.values"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p0bQB8uOv7xT"},"outputs":[],"source":["def split_dataset_into_seq(dataset, start_index=0, end_index=None, history_size=13, step=1):\n","    '''split the dataset to have sequence of observations of length history size'''\n","    data = []\n","    start_index = start_index + history_size\n","    if end_index is None:\n","        end_index = len(dataset)\n","    for i in range(start_index, end_index):\n","        indices = range(i - history_size, i, step)\n","        data.append(dataset[indices])\n","    return np.array(data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ajb5bl5bvtkh"},"outputs":[],"source":["def split_dataset(data, TRAIN_SPLIT=0.7, VAL_SPLIT=0.5, save_path=None):\n","    '''split the dataset into train, val and test splits'''\n","    # normalization\n","    data_mean = data.mean(axis=0)\n","    data_std = data.std(axis=0)\n","    data = (data - data_mean) / data_std\n","    stats = (data_mean, data_std)\n","\n","    data_in_seq = split_dataset_into_seq(data, start_index=0, end_index=None, history_size=13, step=1)\n","\n","    # split between validation dataset and test set:\n","    train_data, val_data = train_test_split(data_in_seq, train_size=TRAIN_SPLIT, shuffle=True, random_state=123)\n","    val_data, test_data = train_test_split(val_data, train_size=VAL_SPLIT, shuffle=True, random_state=123)\n","\n","    return train_data, val_data, test_data"]},{"cell_type":"markdown","source":["#Import the code "],"metadata":{"id":"tsPPCsX202O0"}},{"cell_type":"markdown","source":["##pickle functions"],"metadata":{"id":"yQuf5Vv4lCox"}},{"cell_type":"code","execution_count":9,"metadata":{"id":"7auNURq1lZky","executionInfo":{"status":"ok","timestamp":1686170792644,"user_tz":420,"elapsed":14,"user":{"displayName":"Bliss Singhal","userId":"00844180374647156561"}}},"outputs":[],"source":["def flatten(names):\n","    new_array = []\n","    for i in range(len(names)):\n","        for j in range(len(names[i])):\n","            new_array.append(names[i][j])\n","    return new_array"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"p781t6XClZk-","executionInfo":{"status":"ok","timestamp":1686170792644,"user_tz":420,"elapsed":12,"user":{"displayName":"Bliss Singhal","userId":"00844180374647156561"}}},"outputs":[],"source":["def save_epochs(epochs, file):\n","    with open(file, 'wb') as file: \n","        pickle.dump(epochs, file)"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"tLGHempClZk_","executionInfo":{"status":"ok","timestamp":1686170792645,"user_tz":420,"elapsed":13,"user":{"displayName":"Bliss Singhal","userId":"00844180374647156561"}}},"outputs":[],"source":["def load_epochs(file_name):\n","    with open(file_name, 'rb') as f:\n","        epochs = pickle.load(f)\n","    return epochs"]},{"cell_type":"markdown","source":["##loading the epoch array and labels"],"metadata":{"id":"RWQ8zio6lLQv"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"gqiYKdS_wokw"},"outputs":[],"source":["epochs_array = load_epochs(\"epoch_files/epochs_with_ica.pkl\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q6Dcs59flZlQ"},"outputs":[],"source":["def make_2d(array):\n","    dim1, dim2, dim3 = array.shape\n","    array = np.reshape(array, (dim1, dim2 * dim3))\n","    return array"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0wSEz-McRQNH"},"outputs":[],"source":["epochs_2d = make_2d(epochs_array)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ffEqqQkrAnmN"},"outputs":[],"source":["def transfer_labels2(time, epoch_labels, padding_time = 0):\n","  #note: time is in seconds\n","  shift_epochs = int(time / 5)\n","  padd_epochs = int(padding_time / 5)\n","  for index in range(len(epoch_labels)):\n","    #doesnt shift epoch labels if the seizure happens too early \n","    if epoch_labels[index] == 1 and index - shift_epochs >= 0:\n","      epoch_labels[index - shift_epochs] = 1\n","      #padding before\n","      for i in range(padd_epochs):\n","        if (index - (shift_epochs + padd_epochs) >= 0):\n","          epoch_labels[index - (shift_epochs + padd_epochs)] = 1\n","      epoch_labels[index] = 0\n","  return epoch_labels"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sN-eY0bBwtCZ"},"outputs":[],"source":["def get_all_epoch_labels(label_folder, time_before, padding_time):\n","  labels = []\n","  for i in range(24):\n","    print(i)\n","    if i == 0 or i == 3 or i == 12 or i == 13 or i == 17:\n","      continue\n","    if i < 10:\n","      f = \"chb0\" + str(i) + \"_labels.pkl\"\n","    else:\n","      f = \"chb\" + str(i) + \"_labels.pkl\"\n","    \n","    file_labels = load_epochs(f\"{label_folder}/{f}\") \n","    file_labels = transfer_labels2(time_before, file_labels, padding_time)\n","    labels.append(file_labels)\n","    #flattening the labels array\n","  labels = flatten(labels)\n","  return labels\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SzYnl6uLpC5H"},"outputs":[],"source":["labels = get_all_epoch_labels(\"label_files\", (40 * 60), 40 * 60)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dDX2HQs9o8iI"},"outputs":[],"source":["save_epochs(labels, \"label_files/40min_transfer_40minpadd.pkl\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S25iBxCvzNE1"},"outputs":[],"source":["labels = np.asarray(labels)"]},{"cell_type":"markdown","source":["##split dataset"],"metadata":{"id":"8OKz51TfljAK"}},{"cell_type":"code","source":["lim1 = 200000\n","lim2 = 215000\n","lim3 = 230000"],"metadata":{"id":"IgUwdyRcmM3n","executionInfo":{"status":"ok","timestamp":1686170601854,"user_tz":420,"elapsed":3,"user":{"displayName":"Bliss Singhal","userId":"00844180374647156561"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["x_train = epochs_array[:lim1]\n","y_train = labels[:lim1]\n","x_val = epochs_array[lim1:lim2]\n","y_val = labels[lim1:lim2]\n","x_test = epochs_array[lim2:lim3]\n","y_test = labels[lim2:lim3]"],"metadata":{"id":"EW7UzDjClil8"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n1ClBYcty4jL"},"outputs":[],"source":["def data_to_dataset(x_train, y_train, x_val, y_val, x_test, y_test, batch_size=32, target_features=list(range(20))):\n","    '''\n","    split each train split into inputs and targets \n","    convert each train split into a tf.dataset\n","    '''\n","    x_train, y_train = split_fn(train_data)\n","    x_val, y_val = split_fn(val_data)\n","    x_test, y_test = split_fn(test_data)\n","    # selecting only the first 20 features for prediction: \n","    y_train = y_train[:, :, target_features]\n","    y_val = y_val[:, :, target_features]\n","    y_test = y_test[:, :, target_features]\n","    train_dataset = torch.utils.data.TensorDataset(x_train, y_train)\n","    val_dataset = torch.utils.data.TensorDataset(x_val, y_val)\n","    test_dataset = torch.utils.data.TensorDataset(x_test, y_test)\n","\n","    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size)\n","    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size)\n","    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)\n","    return train_loader, val_loader, test_loader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RzPtGrn4V0WG"},"outputs":[],"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","source":["train_data, val_data, test_data = split_dataset(data)"],"metadata":{"id":"XBrHuuxnmvMS"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VDzKvost8VUu"},"outputs":[],"source":["\n","train_dataset, val_dataset, test_dataset = data_to_dataset(x_train, y_train, x_val, y_val, x_test, y_test)"]},{"cell_type":"markdown","metadata":{"id":"tGacA_fS85sz"},"source":["### Implementation of the Transformer model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8KH-wFTB4e7I"},"outputs":[],"source":["import torch.nn as nn\n","import math\n","\n","class MultiHeadAttention(nn.Module):\n","    '''Multi-head self-attention module'''\n","    def __init__(self, D, H):\n","        super(MultiHeadAttention, self).__init__()\n","        self.H = H # number of heads\n","        self.D = D # dimension\n","        \n","        self.wq = nn.Linear(D, D*H)\n","        self.wk = nn.Linear(D, D*H)\n","        self.wv = nn.Linear(D, D*H)\n","\n","        self.dense = nn.Linear(D*H, D)\n","\n","    def concat_heads(self, x):\n","        '''(B, H, S, D) => (B, S, D*H)'''\n","        B, H, S, D = x.shape\n","        x = x.permute((0, 2, 1, 3)).contiguous()  # (B, S, H, D)\n","        x = x.reshape((B, S, H*D))   # (B, S, D*H)\n","        return x\n","\n","    def split_heads(self, x):\n","        '''(B, S, D*H) => (B, H, S, D)'''\n","        B, S, D_H = x.shape\n","        x = x.reshape(B, S, self.H, self.D)    # (B, S, H, D)\n","        x = x.permute((0, 2, 1, 3))  # (B, H, S, D)\n","        return x\n","\n","    def forward(self, x, mask):\n","\n","        q = self.wq(x)  # (B, S, D*H)\n","        k = self.wk(x)  # (B, S, D*H)\n","        v = self.wv(x)  # (B, S, D*H)\n","\n","        q = self.split_heads(q)  # (B, H, S, D)\n","        k = self.split_heads(k)  # (B, H, S, D)\n","        v = self.split_heads(v)  # (B, H, S, D)\n","\n","        attention_scores = torch.matmul(q, k.transpose(-1, -2)) #(B,H,S,S)\n","        attention_scores = attention_scores / math.sqrt(self.D)\n","\n","        # add the mask to the scaled tensor.\n","        if mask is not None:\n","            attention_scores += (mask * -1e9)\n","        \n","        attention_weights = nn.Softmax(dim=-1)(attention_scores)\n","        scaled_attention = torch.matmul(attention_weights, v)  # (B, H, S, D)\n","        concat_attention = self.concat_heads(scaled_attention) # (B, S, D*H)\n","        output = self.dense(concat_attention)  # (B, S, D)\n","\n","        return output, attention_weights"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SCywNnTvV0WH"},"outputs":[],"source":["B, S, H, D = 9, 11, 5, 8\n","mha = MultiHeadAttention(D, H)\n","out, att = mha.forward(torch.zeros(B, S, D), mask=None)\n","out.shape, att.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bsiqoBF15qxb"},"outputs":[],"source":["# Positional encodings\n","def get_angles(pos, i, D):\n","    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(D))\n","    return pos * angle_rates\n","\n","\n","def positional_encoding(D, position=20, dim=3, device=device):\n","    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n","                            np.arange(D)[np.newaxis, :],\n","                            D)\n","    # apply sin to even indices in the array; 2i\n","    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n","    # apply cos to odd indices in the array; 2i+1\n","    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n","    if dim == 3:\n","        pos_encoding = angle_rads[np.newaxis, ...]\n","    elif dim == 4:\n","        pos_encoding = angle_rads[np.newaxis,np.newaxis,  ...]\n","    return torch.tensor(pos_encoding, device=device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZW26NwrB6LtR"},"outputs":[],"source":["# function that implement the look_ahead mask for masking future time steps. \n","def create_look_ahead_mask(size, device=device):\n","    mask = torch.ones((size, size), device=device)\n","    mask = torch.triu(mask, diagonal=1)\n","    return mask  # (size, size)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PRX4XL52V0WH"},"outputs":[],"source":["create_look_ahead_mask(6)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KUalYf-dtYwb"},"outputs":[],"source":["class TransformerLayer(nn.Module):\n","    def __init__(self, D, H, hidden_mlp_dim, dropout_rate):\n","        super(TransformerLayer, self).__init__()\n","        self.dropout_rate = dropout_rate\n","        self.mlp_hidden = nn.Linear(D, hidden_mlp_dim)\n","        self.mlp_out = nn.Linear(hidden_mlp_dim, D)\n","        self.layernorm1 = nn.LayerNorm(D, eps=1e-9)\n","        self.layernorm2 = nn.LayerNorm(D, eps=1e-9)\n","        self.dropout1 = nn.Dropout(dropout_rate)\n","        self.dropout2 = nn.Dropout(dropout_rate)\n","\n","        self.mha = MultiHeadAttention(D, H)\n","\n","\n","    def forward(self, x, look_ahead_mask):\n","        \n","        attn, attn_weights = self.mha(x, look_ahead_mask)  # (B, S, D)\n","        attn = self.dropout1(attn) # (B,S,D)\n","        attn = self.layernorm1(attn + x) # (B,S,D)\n","\n","        mlp_act = torch.relu(self.mlp_hidden(attn))\n","        mlp_act = self.mlp_out(mlp_act)\n","        mlp_act = self.dropout2(mlp_act)\n","        \n","        output = self.layernorm2(mlp_act + attn)  # (B, S, D)\n","\n","        return output, attn_weights"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gpepObgvV0WI"},"outputs":[],"source":["dl = TransformerLayer(16, 3, 32, 0.1)\n","out, attn = dl(x=torch.zeros(5, 7, 16), look_ahead_mask=None)\n","out.shape, attn.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"04VLiWfcuD4d"},"outputs":[],"source":["class Transformer(nn.Module):\n","    '''Transformer Decoder Implementating several Decoder Layers.\n","    '''\n","    def __init__(self, num_layers, D, H, hidden_mlp_dim, inp_features, out_features, dropout_rate):\n","        super(Transformer, self).__init__()\n","        self.sqrt_D = torch.tensor(math.sqrt(D))\n","        self.num_layers = num_layers\n","        self.input_projection = nn.Linear(inp_features, D) # multivariate input\n","        self.output_projection = nn.Linear(D, out_features) # multivariate output\n","        self.pos_encoding = positional_encoding(D)\n","        self.dec_layers = nn.ModuleList([TransformerLayer(D, H, hidden_mlp_dim, \n","                                        dropout_rate=dropout_rate\n","                                       ) for _ in range(num_layers)])\n","        self.dropout = nn.Dropout(dropout_rate)\n","\n","    def forward(self, x, mask):\n","        B, S, D = x.shape\n","        attention_weights = {}\n","        x = self.input_projection(x)\n","        x *= self.sqrt_D\n","        \n","        x += self.pos_encoding[:, :S, :]\n","\n","        x = self.dropout(x)\n","\n","        for i in range(self.num_layers):\n","            x, block = self.dec_layers[i](x=x,\n","                                          look_ahead_mask=mask)\n","            attention_weights['decoder_layer{}'.format(i + 1)] = block\n","        \n","        x = self.output_projection(x)\n","        \n","        return x, attention_weights # (B,S,S)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qD8WG0B--V1x"},"outputs":[],"source":["# Test Forward pass on the Transformer: \n","transformer = Transformer(num_layers=1, D=32, H=1, hidden_mlp_dim=32,\n","                                       inp_features=28, out_features=20, dropout_rate=0.1)\n","transformer.to(device)\n","(inputs, targets) = next(iter(train_dataset))\n","                         \n","S = inputs.shape[1]\n","mask = create_look_ahead_mask(S)\n","out, attn = transformer (x=inputs, mask=mask)\n","out.shape, attn[\"decoder_layer1\"].shape"]},{"cell_type":"markdown","metadata":{"id":"WzrZUFZz6ia_"},"source":["## Training the Transformer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j5KjICR3V0WJ"},"outputs":[],"source":["param_sizes = [p.numel() for p in transformer.parameters()]\n","print(f\"number of weight/biases matrices: {len(param_sizes)} \"\n","      f\"for a total of {np.sum(param_sizes)} parameters \")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yPNB1c-oV0WJ"},"outputs":[],"source":["transformer = Transformer(num_layers=1, D=32, H=4, hidden_mlp_dim=32,\n","                          inp_features=28, out_features=20, dropout_rate=0.1).to(device)\n","optimizer = torch.optim.RMSprop(transformer.parameters(), \n","                                lr=0.00005)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b1Q5t-L0V0WJ"},"outputs":[],"source":["from tqdm import tqdm\n","\n","n_epochs = 20\n","niter = len(train_dataset)\n","losses, val_losses = [], []\n","\n","for e in tqdm(range(n_epochs)):\n","    \n","    # one epoch on train set\n","    transformer.train()\n","    sum_train_loss = 0.0\n","    for x,y in train_dataset:\n","        S = x.shape[1]\n","        mask = create_look_ahead_mask(S)\n","        out, _ = transformer(x, mask)\n","        loss = torch.nn.MSELoss()(out, y)\n","        sum_train_loss += loss.item()\n","        loss.backward()\n","        optimizer.step()\n","    losses.append(sum_train_loss / niter)\n","    \n","    # Evaluate on val set\n","    transformer.eval()\n","    sum_val_loss = 0.0\n","    for i, (x, y) in enumerate(val_dataset):\n","        S = x.shape[1]\n","        mask = create_look_ahead_mask(S)\n","        out, _ = transformer(x, mask)\n","        loss = torch.nn.MSELoss()(out, y)\n","        sum_val_loss += loss.item()\n","    val_losses.append(sum_val_loss / (i + 1))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-nTKZdg9V0WJ"},"outputs":[],"source":["plt.plot(losses)\n","plt.plot(val_losses);"]},{"cell_type":"markdown","metadata":{"id":"ALeWAQDFnAGx"},"source":["### Evaluation on Test set"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zRJJ-39R257V"},"outputs":[],"source":["test_losses, test_preds  = [], []\n","transformer.eval()\n","for (x, y) in test_dataset:\n","    S = x.shape[-2]\n","    y_pred, _ = transformer(x,\n","                            mask=create_look_ahead_mask(S))\n","    loss_test = torch.nn.MSELoss()(y_pred, y)  # (B,S)\n","    test_losses.append(loss_test.item())\n","    test_preds.append(y_pred.detach().cpu().numpy())\n","test_preds = np.vstack(test_preds)\n","np.mean(test_losses)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k1VCvxaklm0k"},"outputs":[],"source":["# Display predictions vs ground truth: \n","# we'll take one random element of the first batch\n","# and display the first feature\n","seq_len = 12\n","index = np.random.randint(32)\n","feature_num = 0\n","\n","x_test, _ = test_dataset.dataset.tensors\n","x_test = x_test[index, :, feature_num].cpu().numpy()\n","pred = test_preds[index, :, feature_num]\n","x = np.linspace(1, seq_len, seq_len)\n","plt.plot(x, pred, 'red', lw=2, label='predictions for sample: {}'.format(index))\n","plt.plot(x, x_test, 'cyan', lw=2, label='ground-truth for sample: {}'.format(index))\n","plt.legend(fontsize=10)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D5EIKhAdV0WK"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1SWFGn_a0ZMVuUrMuRQO9qy1AL5ILk00M","timestamp":1686090438136},{"file_id":"https://github.com/charlesollion/dlexperiments/blob/master/7-Transformers-Timeseries/Transformers_for_timeseries.ipynb","timestamp":1677173677290}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false}},"nbformat":4,"nbformat_minor":0}